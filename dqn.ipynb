{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from utils import initialize_model, train_stepLR, train_cosine, perform_train\n",
    "import gym\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torch.optim as optim\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device for PyTorch computations\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Environments in Gym version 0.25.2 : 996\n"
     ]
    }
   ],
   "source": [
    "all_env = list(gym.envs.registry.keys())\n",
    "\n",
    "print('Total Environments in Gym version {} : {}'.format(gym.__version__,len(all_env)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation shape: (210, 160, 3)\n",
      "Observation dtype: uint8\n",
      "Number of possible actions: 7\n"
     ]
    }
   ],
   "source": [
    "#define environment\n",
    "env = gym.make(\"AssaultDeterministic-v4\", render_mode=\"rgb_array\")\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "# Inspect the observation\n",
    "print(\"Observation shape:\", obs.shape)\n",
    "print(\"Observation dtype:\", obs.dtype)\n",
    "\n",
    "#Inspect action space\n",
    "num_actions = env.action_space.n\n",
    "print(\"Number of possible actions:\", num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we define the hyper parameters:\n",
    "learning_rate = 0.0001\n",
    "gamma = 0.99                      # Discount factor\n",
    "epsilon = 1.0                     # Initial exploration rate\n",
    "epsilon_decay = 0.995             # Epsilon decay rate\n",
    "epsilon_min = 0.1                 # Minimum epsilon\n",
    "replay_buffer_size = 100000       # Replay buffer size\n",
    "batch_size = 1                  # Batch size for training\n",
    "target_update_frequency = 1000    # Update target network after these many steps\n",
    "max_episodes = 1000               # Maximum number of episodes\n",
    "max_steps_per_episode = 10000     # Maximum steps per episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the Q-Network based on the paper architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        \"\"\"\n",
    "        Initialize the DQN network.\n",
    "        \n",
    "        :param input_shape: Tuple representing the input shape (channels, height, width).\n",
    "        :param num_actions: Number of possible actions (output Q-values for each action).\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        # First convolutional layer: 32 filters of size 8x8 with stride 4\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_shape[0], out_channels=32, kernel_size=8, stride=4)\n",
    "        \n",
    "        # Second convolutional layer: 64 filters of size 4x4 with stride 2\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        \n",
    "        # Third convolutional layer: 64 filters of size 3x3 with stride 1\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        \n",
    "        # Calculate the flattened size after convolutions\n",
    "        def conv_output_size(size, kernel_size, stride, padding=0):\n",
    "            return (size - kernel_size + 2 * padding) // stride + 1\n",
    "\n",
    "        conv_h = conv_output_size(\n",
    "            conv_output_size(conv_output_size(input_shape[1], 8, 4), 4, 2), 3, 1\n",
    "        )\n",
    "        conv_w = conv_output_size(\n",
    "            conv_output_size(conv_output_size(input_shape[2], 8, 4), 4, 2), 3, 1\n",
    "        )\n",
    "        linear_input_size = conv_h * conv_w * 64\n",
    "\n",
    "        # Fully connected layer for output Q-values\n",
    "        self.fc = nn.Linear(linear_input_size, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        :param x: Input tensor of shape (batch_size, channels, height, width).\n",
    "        :return: Tensor of shape (batch_size, num_actions) representing Q-values.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.contiguous().view(x.size(0), -1)  # Flatten for the fully connected layer\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre processing the observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_observation(obs):\n",
    "    \"\"\"\n",
    "    Preprocesses an Atari observation from (210, 160, 3) to (84, 84, 1) grayscale.\n",
    "\n",
    "    :param obs: Raw observation from the environment (NumPy array of shape (210, 160, 3)).\n",
    "    :return: Preprocessed observation (NumPy array of shape (84, 84, 1)).\n",
    "    \"\"\"\n",
    "    # Convert to grayscale\n",
    "    gray_obs = rgb2gray(obs)  # Shape: (210, 160)\n",
    "    \n",
    "    # Resize to 84x84\n",
    "    resized_obs = resize(gray_obs, (84, 84), anti_aliasing=True)  # Shape: (84, 84)\n",
    "    \n",
    "    # Normalize pixel values to [0, 1]\n",
    "    normalized_obs = resized_obs / 255.0\n",
    "    \n",
    "    # Add a channel dimension to make it (84, 84, 1)\n",
    "    #preprocessed_obs = np.expand_dims(normalized_obs, axis=-1)\n",
    "    \n",
    "    return normalized_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"\n",
    "        Initialize the replay buffer.\n",
    "\n",
    "        :param capacity: Maximum number of transitions the buffer can hold.\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0  # Tracks the next position to overwrite when the buffer is full\n",
    "\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Store a transition in the replay buffer.\n",
    "\n",
    "        :param state: Current state (preprocessed).\n",
    "        :param action: Action taken.\n",
    "        :param reward: Reward received.\n",
    "        :param next_state: Next state (preprocessed).\n",
    "        :param done: Whether the episode is done.\n",
    "        \"\"\"\n",
    "        # Create a tuple for the transition\n",
    "        transition = (state, action, reward, next_state, done)\n",
    "\n",
    "        # If the buffer isn't full, add the transition\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(transition)\n",
    "        else:\n",
    "            # Overwrite the oldest transition\n",
    "            self.buffer[self.position] = transition\n",
    "\n",
    "        # Update the position to overwrite\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "    \n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    \n",
    "\n",
    "    def size(self):\n",
    "\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, env, replay_buffer, input_shape, num_actions, batch_size=32, gamma=0.99, lr=0.0001, target_update_frequency=1000, n_frames=1):\n",
    "        \"\"\"\n",
    "        Initialize the DQN agent.\n",
    "\n",
    "        :param env: The Gym environment.\n",
    "        :param replay_buffer: ReplayBuffer instance for experience storage.\n",
    "        :param input_shape: Shape of the input state (e.g., (1, 84, 84)).\n",
    "        :param num_actions: Number of possible actions in the environment.\n",
    "        :param batch_size: Batch size for training.\n",
    "        :param gamma: Discount factor for future rewards.\n",
    "        :param lr: Learning rate for the optimizer.\n",
    "        :param target_update_frequency: Steps between target network updates.\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.target_update_frequency = target_update_frequency\n",
    "        self.num_actions = num_actions\n",
    "        self.n_frames = n_frames\n",
    "\n",
    "        # Q-network and target network\n",
    "        self.q_network = DQN(input_shape, num_actions).to(device)\n",
    "        self.target_network = DQN(input_shape, num_actions).to(device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())  # Initialize with same weights\n",
    "        self.target_network.eval()  # Target network doesn't train\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "\n",
    "        # Epsilon-greedy parameters\n",
    "        self.epsilon = 1.0  # Start with full exploration\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.1\n",
    "\n",
    "        # Training step counter\n",
    "        self.step_count = 0\n",
    "        \n",
    "        # Initialize the frame stack\n",
    "        self.frame_stack = []\n",
    "        \n",
    "    \n",
    "    def reset_frame_stack(self):\n",
    "        \"\"\"Reset the frame stack.\"\"\"\n",
    "        initial_frame = preprocess_observation(self.env.reset()) \n",
    "        self.frame_stack = [initial_frame] * self.n_frames  # Duplicate the first frame\n",
    "\n",
    "    def stack_frames(self, new_frame):\n",
    "        \"\"\"Update the frame stack with a new frame.\"\"\"\n",
    "        self.frame_stack.pop(0)  # Remove the oldest frame\n",
    "        self.frame_stack.append(new_frame)  # Add the new frame\n",
    "        \n",
    "        return np.stack(self.frame_stack, axis=0)\n",
    "\n",
    "    def select_action(self, stacked_state):\n",
    "        \"\"\"\n",
    "        Select an action using epsilon-greedy policy.\n",
    "\n",
    "        :param state: Current state of the environment.\n",
    "        :return: Chosen action.\n",
    "        \"\"\"\n",
    "        \n",
    "        #state = self.stack_frames(state)\n",
    "        \n",
    "        if random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()  # Explore: random action\n",
    "        else:\n",
    "            state_tensor = torch.tensor(stacked_state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            #state_tensor = state_tensor.permute(0, 3, 1, 2) \n",
    "            #print(\"after permute state_tensor\", state_tensor.size())\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(state_tensor)\n",
    "            return torch.argmax(q_values).item()  # Exploit: max Q-value action\n",
    "        \n",
    "\n",
    "    def train_step(self):\n",
    "        \"\"\"\n",
    "        Perform one training step.\n",
    "        \"\"\"\n",
    "        if self.replay_buffer.size() < self.batch_size:\n",
    "            return  # Not enough data to train\n",
    "        \n",
    "        # Sample a batch from the replay buffer\n",
    "        batch = self.replay_buffer.sample(self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Sample a batch from the replay buffer\n",
    "        #single_transition = replay_buffer.sample(batch_size=1)[0]  \n",
    "        #state, action, reward, next_state, done = single_transition\n",
    "\n",
    "        # Convert to tensors\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32).to(device)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64).to(device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32).to(device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Compute current Q-values\n",
    "        #print(\"Initial state print before permutation \", states.shape)\n",
    "        #states = states.permute(0, 3, 1, 2)  \n",
    "        #print(\"Initial state print after permutation \", states.shape)\n",
    "        \n",
    "        q_values = self.q_network(states)\n",
    "        #print(\"Q_values size\", q_values.size())\n",
    "        #actions = actions.unsqueeze(0) \n",
    "        #print(\"action size\", actions.size())\n",
    "        q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        \n",
    "        #formating for the network\n",
    "        #print(\"next state before permutation \", next_states.size())\n",
    "\n",
    "        # Compute target Q-values\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1)[0]\n",
    "        target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "\n",
    "        # Compute loss and backpropagate\n",
    "        loss = torch.nn.functional.mse_loss(q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update target network\n",
    "        self.step_count += 1\n",
    "        if self.step_count % self.target_update_frequency == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(agent, num_episodes, max_steps_per_episode, csv_filename):\n",
    "    \"\"\"\n",
    "    Train the DQN agent.\n",
    "\n",
    "    :param agent: The DQNAgent instance.\n",
    "    :param num_episodes: Number of episodes to train.\n",
    "    :param max_steps_per_episode: Maximum steps per episode.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the CSV file with a header\n",
    "    with open(csv_filename, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"episode\", \"total_reward\", \"loss\", \"epsilon\"])  # Write the header row\n",
    "        \n",
    "        \n",
    "    for episode in range(num_episodes):\n",
    "        \n",
    "        # Start the stacked frames\n",
    "        agent.reset_frame_stack()\n",
    "        agent_reset= agent.env.reset()\n",
    "        #print(agent_reset.shape)\n",
    "        initial_frame = preprocess_observation(agent_reset) # Preprocess the initial observation\n",
    "        #print(\"Initial frame \", initial_frame.shape)\n",
    "        state = agent.stack_frames(initial_frame)  \n",
    "        \n",
    "        \n",
    "        total_reward = 0\n",
    "        loss = 0\n",
    "\n",
    "        for step in range(max_steps_per_episode):\n",
    "            # Select an action using the epsilon-greedy policy\n",
    "            action = agent.select_action(state)\n",
    "            \n",
    "            # Take the action in the environment\n",
    "            next_frame, reward, done, _ = agent.env.step(action)\n",
    "            next_frame = preprocess_observation(next_frame)\n",
    "            #print(\"frame \", next_frame.shape)\n",
    "\n",
    "            # Update the frame stack with the new frame\n",
    "            next_state = agent.stack_frames(next_frame)\n",
    "            #print(next_state.shape)\n",
    "\n",
    "            # Store the transition in the replay buffer\n",
    "            agent.replay_buffer.store(state, action, reward, next_state, done)\n",
    "\n",
    "            # Train the network\n",
    "            loss = agent.train_step()\n",
    "\n",
    "            # Update total reward\n",
    "            total_reward += reward\n",
    "\n",
    "            # Update the current state\n",
    "            state = next_state\n",
    "\n",
    "            # Break the loop if the episode is done\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Decay epsilon\n",
    "        agent.epsilon = max(agent.epsilon * agent.epsilon_decay, agent.epsilon_min)\n",
    "        \n",
    "         # Save results to CSV\n",
    "        with open(csv_filename, mode=\"a\", newline=\"\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([episode + 1, total_reward, loss, agent.epsilon])  # Append the row\n",
    "\n",
    "        print(f\"Episode {episode + 1}/{num_episodes} - Reward: {total_reward:.2f}, Loss: {loss:.4f}, Epsilon: {agent.epsilon:.4f}\")\n",
    "\n",
    "        \n",
    "    print(f\"Training results saved to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/1000 - Reward: 336.00, Loss: 13.0383, Epsilon: 0.9950\n",
      "Episode 2/1000 - Reward: 399.00, Loss: 26.6541, Epsilon: 0.9900\n",
      "Episode 3/1000 - Reward: 168.00, Loss: 13.5869, Epsilon: 0.9851\n",
      "Episode 4/1000 - Reward: 168.00, Loss: 0.4905, Epsilon: 0.9801\n",
      "Episode 5/1000 - Reward: 231.00, Loss: 0.1777, Epsilon: 0.9752\n",
      "Episode 6/1000 - Reward: 252.00, Loss: 0.0997, Epsilon: 0.9704\n",
      "Episode 7/1000 - Reward: 315.00, Loss: 13.5983, Epsilon: 0.9655\n",
      "Episode 8/1000 - Reward: 294.00, Loss: 0.1660, Epsilon: 0.9607\n",
      "Episode 9/1000 - Reward: 252.00, Loss: 13.6294, Epsilon: 0.9559\n",
      "Episode 10/1000 - Reward: 252.00, Loss: 13.4894, Epsilon: 0.9511\n",
      "Episode 11/1000 - Reward: 273.00, Loss: 0.1416, Epsilon: 0.9464\n",
      "Episode 12/1000 - Reward: 189.00, Loss: 0.0726, Epsilon: 0.9416\n",
      "Episode 13/1000 - Reward: 399.00, Loss: 26.2343, Epsilon: 0.9369\n",
      "Episode 14/1000 - Reward: 252.00, Loss: 0.2070, Epsilon: 0.9322\n",
      "Episode 15/1000 - Reward: 294.00, Loss: 26.8117, Epsilon: 0.9276\n",
      "Episode 16/1000 - Reward: 84.00, Loss: 13.5075, Epsilon: 0.9229\n",
      "Episode 17/1000 - Reward: 168.00, Loss: 0.4960, Epsilon: 0.9183\n",
      "Episode 18/1000 - Reward: 189.00, Loss: 0.4208, Epsilon: 0.9137\n",
      "Episode 19/1000 - Reward: 294.00, Loss: 0.1771, Epsilon: 0.9092\n",
      "Episode 20/1000 - Reward: 273.00, Loss: 13.2260, Epsilon: 0.9046\n",
      "Episode 21/1000 - Reward: 231.00, Loss: 2.6563, Epsilon: 0.9001\n",
      "Episode 22/1000 - Reward: 294.00, Loss: 0.0502, Epsilon: 0.8956\n",
      "Episode 23/1000 - Reward: 420.00, Loss: 0.4841, Epsilon: 0.8911\n",
      "Episode 24/1000 - Reward: 126.00, Loss: 26.4415, Epsilon: 0.8867\n",
      "Episode 25/1000 - Reward: 147.00, Loss: 0.2915, Epsilon: 0.8822\n",
      "Episode 26/1000 - Reward: 315.00, Loss: 0.2242, Epsilon: 0.8778\n",
      "Episode 27/1000 - Reward: 315.00, Loss: 0.4125, Epsilon: 0.8734\n",
      "Episode 28/1000 - Reward: 210.00, Loss: 40.6827, Epsilon: 0.8691\n",
      "Episode 29/1000 - Reward: 252.00, Loss: 13.9184, Epsilon: 0.8647\n",
      "Episode 30/1000 - Reward: 273.00, Loss: 0.1738, Epsilon: 0.8604\n",
      "Episode 31/1000 - Reward: 315.00, Loss: 13.5509, Epsilon: 0.8561\n",
      "Episode 32/1000 - Reward: 273.00, Loss: 13.6729, Epsilon: 0.8518\n",
      "Episode 33/1000 - Reward: 294.00, Loss: 0.4531, Epsilon: 0.8475\n",
      "Episode 34/1000 - Reward: 210.00, Loss: 0.0328, Epsilon: 0.8433\n",
      "Episode 35/1000 - Reward: 168.00, Loss: 13.1191, Epsilon: 0.8391\n",
      "Episode 36/1000 - Reward: 168.00, Loss: 0.0923, Epsilon: 0.8349\n",
      "Episode 37/1000 - Reward: 189.00, Loss: 19.9887, Epsilon: 0.8307\n",
      "Episode 38/1000 - Reward: 147.00, Loss: 0.5265, Epsilon: 0.8266\n",
      "Episode 39/1000 - Reward: 147.00, Loss: 26.9663, Epsilon: 0.8224\n"
     ]
    }
   ],
   "source": [
    "replay_buffer = ReplayBuffer(capacity=100000)\n",
    "csv_filename = \"dqn_results_4frames.csv\"\n",
    "n_frames = 4\n",
    "\n",
    "# Initialize agent\n",
    "input_shape = (n_frames, 84, 84)  # 4 stacked frames\n",
    "num_actions = env.action_space.n\n",
    "agent = DQNAgent(env, replay_buffer, input_shape, num_actions, n_frames=n_frames)\n",
    "\n",
    "# Train the agent\n",
    "num_episodes = 1000\n",
    "max_steps_per_episode = 10000\n",
    "train_dqn(agent, num_episodes, max_steps_per_episode, csv_filename=csv_filename)\n",
    "\n",
    "# Evaluate the agent\n",
    "#evaluate_agent(agent, num_episodes=10, max_steps_per_episode=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV results\n",
    "df = pd.read_csv(\"dqn_results_4frames.csv\")\n",
    "\n",
    "# Compute averages of total rewards every 50 episodes\n",
    "df[\"group\"] = df[\"episode\"] // 50  # Group by every 50 episodes\n",
    "average_rewards = df.groupby(\"group\")[\"total_reward\"].mean()  # Average rewards\n",
    "average_episodes = df.groupby(\"group\")[\"episode\"].mean()  # Midpoint for episodes\n",
    "\n",
    "# Plot the average total rewards over grouped episodes\n",
    "plt.plot(average_episodes, average_rewards, label=\"DQN 10-frames\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average Total Reward\")\n",
    "plt.title(\"Average Total Reward\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent, num_episodes, max_steps_per_episode):\n",
    "    \"\"\"\n",
    "    Evaluate the DQN agent with stacked frames.\n",
    "\n",
    "    :param agent: The DQNAgent instance.\n",
    "    :param num_episodes: Number of evaluation episodes.\n",
    "    :param max_steps_per_episode: Maximum steps per episode.\n",
    "    \"\"\"\n",
    "    agent.epsilon = 0.0  # Turn off exploration\n",
    "    total_rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        # Reset the environment and initialize the frame stack\n",
    "        agent.reset_frame_stack()\n",
    "        initial_frame = preprocess_observation(agent.env.reset())\n",
    "        state = agent.stack_frames(initial_frame)  # Initialize the stacked state\n",
    "\n",
    "        total_reward = 0\n",
    "\n",
    "        for step in range(max_steps_per_episode):\n",
    "            # Select the best action\n",
    "            action = agent.select_action(state)\n",
    "\n",
    "            # Take the action in the environment\n",
    "            next_frame, reward, done, _ = agent.env.step(action)\n",
    "            next_frame = preprocess_observation(next_frame)\n",
    "\n",
    "            # Update the frame stack with the new frame\n",
    "            state = agent.stack_frames(next_frame)\n",
    "\n",
    "            # Update the total reward\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "        print(f\"Evaluation Episode {episode + 1}/{num_episodes} - Reward: {total_reward:.2f}\")\n",
    "\n",
    "    print(f\"Average Reward over {num_episodes} Episodes: {np.mean(total_rewards):.2f}\")\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_agent(agent, num_episodes=10, max_steps_per_episode=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
